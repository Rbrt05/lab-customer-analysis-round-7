{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f7fd5f",
   "metadata": {},
   "source": [
    "## Lab | Customer Analysis Round 2\n",
    "\n",
    "\n",
    "For this lab, we will be using the marketing_customer_analysis.csv file that you can find in the files_for_lab folder. Check out the files_for_lab/about.md to get more information if you are using the Online Excel.\n",
    "\n",
    "Note: For the next labs we will be using the same data file. Please save the code, so that you can re-use it later in the labs following this lab.\n",
    "\n",
    "#### Dealing with the data\n",
    "- Show the dataframe shape.\n",
    "- Standardize header names.\n",
    "- Which columns are numerical?\n",
    "- Which columns are categorical?\n",
    "- Check and deal with NaN values.\n",
    "- Datetime format - Extract the months from the dataset and store in a separate column. Then filter the data to show only the information for the first quarter , ie. January, February and March. Hint: If data from March does not exist, consider only January and February.\n",
    "- BONUS: Put all the previously mentioned data transformations into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dbbbb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9134, 24)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('files_for_lab/csv_files/marketing_customer_analysis.csv')\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e92756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "col=[]\n",
    "for columns in data.columns:\n",
    "    col.append(columns.lower())\n",
    "data.columns=col\n",
    "\n",
    "\n",
    "list(data.columns)\n",
    "\n",
    "data.columns = data.columns\n",
    "\n",
    "data=data.rename(columns={'employmentstatus':'employment status'})\n",
    "\n",
    "list(data.columns)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59827389",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad44d3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes(['int64','float64'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec322ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes(['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc42c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93da43ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.copy()\n",
    "\n",
    "data2= data2[data2['state'].isna()==False]\n",
    "\n",
    "data2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2077519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for \"month since last claim\" it might make sense, that value is NULL since there might never been one -> keep it\n",
    "\n",
    "# number of open complaints NaN -> set to 0\n",
    "data2['number of open complaints'] = data2['number of open complaints'].fillna(0)\n",
    "\n",
    "# vehicle type ignore - missing values since it's either A or empty\n",
    "## data2['vehicle type'].unique()\n",
    "\n",
    "# depending on needs we can delete rows with missing values for vehicle class/vehicle size\n",
    "data2['vehicle class'].unique()\n",
    "data2['vehicle size'].unique()\n",
    "\n",
    "# Executed for data3\n",
    "data3= data2[data2['vehicle class'].isna()==False]\n",
    "\n",
    "data3.isna().sum()\n",
    "data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ed040",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3['effective to date'] = pd.to_datetime(data3['effective to date'],errors='coerce')\n",
    "\n",
    "data3['month'] = data3['effective to date'].dt.month\n",
    "\n",
    "display(data3[['month']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d70b69",
   "metadata": {},
   "source": [
    "print(df.shape)\n",
    "\n",
    "df.columns=[col.lower() for col in df.columns]\n",
    "\n",
    "print(\"The numerical columns are: \", df.select_dtypes(np.number))\n",
    "\n",
    "print(\"The numerical columns are: \", df.select_dtypes(np.object))\n",
    "\n",
    "print(\"The df contains this missing values: \", df.isna().sum())\n",
    "\n",
    "df['effective to date'] = pd.to_datetime(df['effective to date'],errors='coerce')\n",
    "\n",
    "df['month'] = df['effective to date'].dt.month\n",
    "\n",
    "display(df[['month']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8217dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bonus (df):\n",
    "    print(df.shape)\n",
    "\n",
    "    df.columns=[col.lower() for col in df.columns]\n",
    "\n",
    "    print(\"The numerical columns are: \", df.select_dtypes(np.number))\n",
    "\n",
    "    print(\"The numerical columns are: \", df.select_dtypes(np.object))\n",
    "\n",
    "    print(\"The df contains this missing values: \", df.isna().sum())\n",
    "\n",
    "    df['effective to date'] = pd.to_datetime(df['effective to date'],errors='coerce') \n",
    "    df['month'] = df['effective to date'].dt.month \n",
    "    display(df[['month']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61279d56",
   "metadata": {},
   "source": [
    "### Lab | Customer Analysis Round 3\n",
    "\n",
    "For this lab, we still keep using the marketing_customer_analysis.csv file. You can find the file in the files_for_lab folder.\n",
    "\n",
    "Get the data\n",
    "Use the same jupyter file from the last lab, Customer Analysis Round 3\n",
    "\n",
    "##### EDA (Exploratory Data Analysis) - Complete the following tasks to explore the data:\n",
    "\n",
    "- Show DataFrame info.\n",
    "- Describe DataFrame.\n",
    "- Show a plot of the total number of responses.\n",
    "- Show a plot of the response rate by the sales channel.\n",
    "- Show a plot of the response rate by the total claim amount.\n",
    "- Show a plot of the response rate by income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0db916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DataFrame info. \n",
    "data2=data.copy()\n",
    "data2.info()\n",
    "data2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe DataFrame.\n",
    "display(data2.describe())\n",
    "display(data2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae29f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95971f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a plot of the total number of responses.\n",
    "print('Number of responses: ',data2[data2['response']=='Yes'].shape[0])\n",
    "print('Number of no response: ',data2[data2['response']=='No'].shape[0])\n",
    "\n",
    "ax = sns.countplot(x='response',data=data2)\n",
    "\n",
    "for p in ax.patches:\n",
    "   ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25,p.get_height()+0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1408e199",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data2, x='response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbda0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a plot of the response rate by the sales channel.\n",
    "\n",
    "ax = sns.countplot(x='sales channel',hue='response', data=data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d139c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#response_rate\n",
    "print(\"The number of responses is: \", data2[data2['response']=='Yes'].shape[0])\n",
    "print(\"The number of no responses is: \",data2[(data2['response'].isin(['Yes','No']))].shape[0])\n",
    "\n",
    "print(\"The average response rate is: \",\n",
    "round((data2[data2['response']=='Yes'].shape[0]/data2[(data2['response'].isin(['Yes','No']))].shape[0])*100,2))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#replace yes and no by 1 and 0\n",
    "\n",
    "data3=data2.copy()\n",
    "data3=data3.replace({'response':{'Yes':1,'No':0}})\n",
    "display(data3.head())\n",
    "\n",
    "list(data3['response'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ec44e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show a plot of the response rate by the total claim amount.\n",
    "\n",
    "a = sns.histplot(data2, x='total claim amount',stat='percent',hue='response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc39d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=data3, x=data3['income'], y=data3['response'].sum()/data3['response'].shape[0])\n",
    "\n",
    "print(data3['response'].sum()/data3['response'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d1f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show a plot of the response rate by income.\n",
    "\n",
    "sns.catplot(x=\"income\",y=\"response\", data=data2, kind='box')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39069342",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"income\",y=\"response\", data=data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202add5e",
   "metadata": {},
   "source": [
    "### Lab | Customer Analysis Round 4\n",
    "\n",
    "In today's lesson we talked about continuous distributions (mainly normal distribution), linear regression and how multicollinearity can impact the model. In this lab, we will test your knowledge on those things using the marketing_customer_analysis.csv file. You have been using the same data in the previous labs (round 2 and 3). You can continue using the same jupyter file. The file can be found in the files_for_lab folder.\n",
    "\n",
    "##### Get the data\n",
    "Use the jupyter file from the last lab (Customer Analysis Round 3)\n",
    "\n",
    "##### Complete the following task\n",
    "\n",
    "- Check the data types of the columns. Get the numeric data into dataframe called numerical and categorical columns in a dataframe called categoricals. (You can use np.number and np.object to select the numerical data types and categorical data types respectively)\n",
    "\n",
    "- Now we will try to check the normality of the numerical variables visually\n",
    "- Use seaborn library to construct distribution plots for the numerical variables\n",
    "- Use Matplotlib to construct histograms\n",
    "- Do the distributions for different numerical variables look like a normal distribution\n",
    "- For the numerical variables, check the multicollinearity between the features. Please note that we will use the column total_claim_amount later as the target variable.\n",
    "- Drop one of the two features that show a high correlation between them (greater than 0.9). Write code for both the correlation matrix and for seaborn heatmap. If there is no pair of features that have a high correlation, then do not drop any features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eca4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the data types of the columns. \n",
    "#Get the numeric data into dataframe called numerical and categorical columns in a dataframe called categoricals. \n",
    "#(You can use np.number and np.object to select the numerical data types and categorical data types respectively)\n",
    "\n",
    "print(data2.dtypes)\n",
    "\n",
    "numericals= data2.select_dtypes(['number'])\n",
    "categoricals= data2.select_dtypes(['object'])\n",
    "\n",
    "display(numericals.head())\n",
    "display(categoricals.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76030f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will try to check the normality of the numerical variables visually\n",
    "    #Use seaborn library to construct distribution plots for the numerical variables\n",
    "    #Use Matplotlib to construct histograms\n",
    "    #Do the distributions for different numerical variables look like a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b294263",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,4,figsize=(15,10)) # now ax = np.array of two elements\n",
    "sns.histplot(data2['customer lifetime value'], ax=ax[0,0])\n",
    "sns.histplot(data2['income'], ax=ax[0,1])\n",
    "sns.histplot(data2['monthly premium auto'], ax=ax[0,2])\n",
    "sns.histplot(data2['months since last claim'], ax=ax[0,3])\n",
    "sns.histplot(data2['months since policy inception'], ax=ax[1,0])\n",
    "sns.histplot(data2['number of open complaints'], ax=ax[1,1])\n",
    "sns.histplot(data2['number of policies'], ax=ax[1,2])\n",
    "sns.histplot(data2['total claim amount'], ax=ax[1,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce5060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)   \n",
    "\n",
    "ax1.hist(x=data2['customer lifetime value'],color='orange')\n",
    "ax1.set_title('customer lifetime value')\n",
    "ax2.hist(x=data2['income'], color='orange' )\n",
    "ax2.set_title('income')\n",
    "ax3.hist(x=data2['monthly premium auto'],color='orange')\n",
    "ax3.set_title('monthly premium auto')\n",
    "ax4.hist(x=data2['months since last claim'],color='orange')\n",
    "ax4.set_title('months since last claim')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2) \n",
    "ax1.hist(x=data2['months since policy inception'],color='orange')\n",
    "ax1.set_title('months since policy inception')\n",
    "ax2.hist(x=data2['number of open complaints'],color='orange')\n",
    "ax2.set_title('number of open complaints')\n",
    "ax3.hist(x=data2['number of policies'],color='orange')\n",
    "ax3.set_title('number of policies')\n",
    "ax4.hist(x=data2['total claim amount'],color='orange')\n",
    "ax4.set_title('total claim amount')\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95cb8ab",
   "metadata": {},
   "source": [
    "The variables are not normally distributed. Most of them are positively skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d5e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the numerical variables, check the multicollinearity between the features. \n",
    "#Please note that we will use the column total_claim_amount later as the target variable.\n",
    "\n",
    "correlations_matrix = numericals.corr()\n",
    "correlations_matrix\n",
    "\n",
    "correlations_matrix = numericals.corr()\n",
    "sns.heatmap(correlations_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop one of the two features that show a high correlation between them (greater than 0.9). \n",
    "#Write code for both the correlation matrix and for seaborn heatmap. \n",
    "#If there is no pair of features that have a high correlation, then do not drop any features\n",
    "\n",
    "# There is no variable with correlation >0.9 that needs to be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545fe041",
   "metadata": {},
   "source": [
    "### Lab | Customer Analysis Round 5\n",
    "\n",
    "For this lab, we still keep using the marketing_customer_analysis.csv file that you can find in the files_for_lab folder.\n",
    "\n",
    "##### Get the data\n",
    "We are using the marketing_customer_analysis.csv file.\n",
    "\n",
    "##### Dealing with the data\n",
    "Already done in the round 2.\n",
    "\n",
    "##### Explore the data\n",
    "Done in the round 3.\n",
    "\n",
    "##### Processing Data\n",
    "(Further processing...)\n",
    "\n",
    "- X-y split.\n",
    "- Normalize (numerical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2d237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X-y split\n",
    "\n",
    "y = numericals['total claim amount']\n",
    "X_num = numericals.drop(['total claim amount'],axis=1)\n",
    "\n",
    "display(y.head())\n",
    "X_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b1a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing (numerical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7644d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler  # sets for each column the minimum = 0 and the maximum = 1\n",
    "MinMaxtransformer = MinMaxScaler()\n",
    "MinMaxtransformer.fit(X_num) # fit () Determine the min and the max of each column \n",
    "\n",
    "x_normalized = MinMaxtransformer.transform(X_num) # .transform() applies the transformation x normalized will be np.array\n",
    "x_normalized = pd.DataFrame(x_normalized,columns=X_num.columns)\n",
    "x_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec02c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,4,figsize=(15,10)) # now ax = np.array of two elements\n",
    "sns.histplot(x_normalized['customer lifetime value'], ax=ax[0,0])\n",
    "sns.histplot(x_normalized['income'], ax=ax[0,1])\n",
    "sns.histplot(x_normalized['monthly premium auto'], ax=ax[0,2])\n",
    "sns.histplot(x_normalized['months since last claim'], ax=ax[0,3])\n",
    "sns.histplot(x_normalized['months since policy inception'], ax=ax[1,0])\n",
    "sns.histplot(x_normalized['number of open complaints'], ax=ax[1,1])\n",
    "sns.histplot(x_normalized['number of policies'], ax=ax[1,2])\n",
    "sns.histplot(x_normalized['number of policies'], ax=ax[1,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090bb012",
   "metadata": {},
   "source": [
    "### Lab | Customer Analysis Round 6\n",
    "\n",
    "For this lab, we still keep using the marketing_customer_analysis.csv file that you can find in the files_for_lab folder.\n",
    "\n",
    "##### Get the data\n",
    "We are using the marketing_customer_analysis.csv file.\n",
    "\n",
    "##### Dealing with the data\n",
    "Already done in the round 2.\n",
    "\n",
    "##### Explore the data\n",
    "Done in the round 3.\n",
    "\n",
    "##### Processing Data\n",
    "(Further processing...)\n",
    "\n",
    "- X-y split. (done)\n",
    "- Normalize (numerical). (done)\n",
    "- One Hot/Label Encoding (categorical).\n",
    "- Concat DataFrames\n",
    "\n",
    "##### Linear Regression\n",
    "- Train-test split.\n",
    "- Apply linear regression.\n",
    "\n",
    "##### Model Validation\n",
    "- R2.\n",
    "- MSE.\n",
    "- RMSE.\n",
    "- MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69316536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot/Label Encoding (categorical).\n",
    "\n",
    "X_cat=categoricals.copy()\n",
    "\n",
    "X_cat.describe()\n",
    "X_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66459b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_location=pd.DataFrame(X_cat[X_cat.columns[8]])\n",
    "\n",
    "display(X_location)\n",
    "\n",
    "X_cat['location code'].unique()\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(X_location) # This determines how many unique values has each categorical column\n",
    "\n",
    "print(encoder.categories_)\n",
    "encoded = encoder.transform(X_location).toarray()\n",
    "X_location_enc = pd.DataFrame(encoded,columns=encoder.categories_)\n",
    "X_location_enc = X_location_enc.drop(['Rural'], axis=1)\n",
    "X_location_enc = X_location_enc.rename(columns={'Suburban' : 'suburban', 'Urban' : 'urban'})\n",
    "X_location_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70111f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gender=pd.DataFrame(X_cat[X_cat.columns[7]])\n",
    "\n",
    "display(X_gender)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(X_gender)\n",
    "X_gender_trans = label_encoder.transform(X_gender) # ordered wrt value counts\n",
    "X_gender_trans = pd.DataFrame(X_gender_trans,columns=X_gender.columns)\n",
    "display(X_gender_trans)\n",
    "X_gender_trans['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a5264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat DataFrames\n",
    "\n",
    "X_new = pd.concat([x_normalized,X_gender_trans, X_location_enc],axis=1)\n",
    "display(X_new.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a4fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split.\n",
    "X_new\n",
    "y\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=100)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c4db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83397afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2.\n",
    "from sklearn.metrics import r2_score\n",
    "y_pred_train = lm.predict(X_train)\n",
    "print(\"r2 for train data: \",r2_score(y_train, y_pred_train))\n",
    "\n",
    "y_pred_test = lm.predict(X_test)\n",
    "print(\"r2 for test data: \",r2_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa4f23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE.\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse=mean_squared_error(y_test,y_pred_test)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14134ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE.\n",
    "rmse=np.sqrt(mean_squared_error(y_test,y_pred_test))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cb0380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE.\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "mae = mean_absolute_error(y_test, y_pred_test)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c2de24",
   "metadata": {},
   "source": [
    "### Lab | Customer Analysis Round 7\n",
    "For this lab, we still keep using the marketing_customer_analysis.csv file that you can find in the files_for_lab folder.\n",
    "\n",
    "Remember the previous rounds. Follow the steps as shown in previous lectures and try to improve the accuracy of the model. Include both categorical columns in the exercise. Some approaches you can try in this exercise:\n",
    "\n",
    "- use the concept of multicollinearity and remove insignificant variables\n",
    "- use a different method of scaling the numerical variables\n",
    "- use a different ratio of train test split\n",
    "- use the transformation on numerical columns which align it more towards a normal distribution\n",
    "\n",
    "##### Get the data\n",
    "We are using the marketing_customer_analysis.csv file.\n",
    "\n",
    "##### Dealing with the data\n",
    "Already done in rounds 2 to 7.\n",
    "\n",
    "##### Bonus: Build a function, from round 2 and round 7, to clean and process the data.\n",
    "\n",
    "Explore the data\n",
    "Done in the round 3.\n",
    "\n",
    "Modeling\n",
    "Description:\n",
    "\n",
    "##### Try to improve the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b846e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------\n",
    "# Model optimization\n",
    "\n",
    "# Normalize Numericals with StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Standardtransformer = StandardScaler()\n",
    "Standardtransformer.fit(X_num) # This will obtain the mean and the sd of each column\n",
    "x_standardized = Standardtransformer.transform(X_num) # Applies the transformation\n",
    "x_standardized = pd.DataFrame(x_standardized,columns=X_num.columns)\n",
    "x_standardized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56ddb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,4,figsize=(15,10)) # now ax = np.array of two elements\n",
    "sns.histplot(x_standardized['customer lifetime value'], ax=ax[0,0])\n",
    "sns.histplot(x_standardized['income'], ax=ax[0,1])\n",
    "sns.histplot(x_standardized['monthly premium auto'], ax=ax[0,2])\n",
    "sns.histplot(x_standardized['months since last claim'], ax=ax[0,3])\n",
    "sns.histplot(x_standardized['months since policy inception'], ax=ax[1,0])\n",
    "sns.histplot(x_standardized['number of open complaints'], ax=ax[1,1])\n",
    "sns.histplot(x_standardized['number of policies'], ax=ax[1,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc153d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Database with numericals based on standard Scaler\n",
    "\n",
    "X_new2 = pd.concat([x_standardized,X_gender_trans, X_location_enc],axis=1)\n",
    "display(X_new2.head())\n",
    "# No Multicollinearity -> no variables to get rid of, based on multicollinearity\n",
    "\n",
    "# Different Train-test split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_new2, y, test_size=0.2, random_state=100)\n",
    "\n",
    "print(X_train2.shape)\n",
    "print(X_test2.shape)\n",
    "print(y_train2.shape)\n",
    "print(y_test2.shape)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lm2 = LinearRegression()\n",
    "lm2.fit(X_train2,y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ff695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2.\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred_train2 = lm2.predict(X_train2)\n",
    "print(\"r2 for train data: \",r2_score(y_train2, y_pred_train2))\n",
    "\n",
    "y_pred_test2 = lm2.predict(X_test2)\n",
    "print(\"r2 for test data: \",r2_score(y_test2, y_pred_test2))\n",
    "\n",
    "\n",
    "# --> Test size seems not to have a big influence\n",
    "# --> DIfferent Scaling Method seems not to have a big influence\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE.\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse=mean_squared_error(y_test2,y_pred_test2)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65abd644",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gg/ydzntksx7jl3t92y3_bqsw140000gn/T/ipykernel_73928/2523519545.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Third option: ....\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_new3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'months since last claim'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'months since policy inception'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'number of open complaints'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'number of policies'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'gender'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_new' is not defined"
     ]
    }
   ],
   "source": [
    "# Third option: ....\n",
    "\n",
    "X_new3 = X_new.drop(['months since last claim','months since policy inception','number of open complaints','number of policies','gender'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a80a5453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a function for all steps of data cleaning\n",
    "\n",
    "def data_cleaning(df):\n",
    "    \n",
    "    # Change Labels\n",
    "    col=[]\n",
    "    for columns in df.columns:\n",
    "        col.append(columns.lower())\n",
    "    df.columns=col\n",
    "    \n",
    "    numericals= df.select_dtypes(['number'])\n",
    "    categoricals= df.select_dtypes(['object'])\n",
    "\n",
    "\n",
    "    # Nomalization of X\n",
    "    X_num = numericals.drop(['total claim amount'],axis=1)\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler  # sets for each column the minimum = 0 and the maximum = 1\n",
    "    MinMaxtransformer = MinMaxScaler()\n",
    "    MinMaxtransformer.fit(X_num) # fit () Determine the min and the max of each column \n",
    "\n",
    "    x_normalized = MinMaxtransformer.transform(X_num) # .transform() applies the transformation x normalized will be np.array\n",
    "    x_normalized = pd.DataFrame(x_normalized,columns=X_num.columns)\n",
    "    x_normalized.head()\n",
    "    \n",
    "    # Encoding Categorical Data.\n",
    "    X_cat=categoricals.copy()\n",
    "\n",
    "    # Location\n",
    "    X_location=pd.DataFrame(X_cat[X_cat.columns[8]])\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    encoder = OneHotEncoder()\n",
    "    encoder.fit(X_location) # This determines how many unique values has each categorical column\n",
    "    encoded = encoder.transform(X_location).toarray()\n",
    "    X_location_enc = pd.DataFrame(encoded,columns=encoder.categories_)\n",
    "    X_location_enc = X_location_enc.drop(['Rural'], axis=1)\n",
    "    X_location_enc = X_location_enc.rename(columns={'Suburban' : 'suburban', 'Urban' : 'urban'})\n",
    "\n",
    "    # Gender\n",
    "    X_gender=pd.DataFrame(X_cat[X_cat.columns[7]])\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(X_gender)\n",
    "    X_gender_trans = label_encoder.transform(X_gender) # ordered wrt value counts\n",
    "    X_gender_trans = pd.DataFrame(X_gender_trans,columns=X_gender.columns)\n",
    "    \n",
    "    # Concat Data Frames again and do use X-y Split for next steps\n",
    "    X_new = pd.concat([x_normalized,X_gender_trans, X_location_enc],axis=1)\n",
    "    y = numericals['total claim amount']\n",
    "    \n",
    "    return display(X_new.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b12d70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertkammerer/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer lifetime value</th>\n",
       "      <th>income</th>\n",
       "      <th>monthly premium auto</th>\n",
       "      <th>months since last claim</th>\n",
       "      <th>months since policy inception</th>\n",
       "      <th>number of open complaints</th>\n",
       "      <th>number of policies</th>\n",
       "      <th>gender</th>\n",
       "      <th>(suburban,)</th>\n",
       "      <th>(urban,)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010629</td>\n",
       "      <td>0.562847</td>\n",
       "      <td>0.033755</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.050505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139241</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.134960</td>\n",
       "      <td>0.487763</td>\n",
       "      <td>0.198312</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.070589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189873</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.656566</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011245</td>\n",
       "      <td>0.438443</td>\n",
       "      <td>0.050633</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer lifetime value    income  monthly premium auto  \\\n",
       "0                 0.010629  0.562847              0.033755   \n",
       "1                 0.062406  0.000000              0.139241   \n",
       "2                 0.134960  0.487763              0.198312   \n",
       "3                 0.070589  0.000000              0.189873   \n",
       "4                 0.011245  0.438443              0.050633   \n",
       "\n",
       "   months since last claim  months since policy inception  \\\n",
       "0                 0.914286                       0.050505   \n",
       "1                 0.371429                       0.424242   \n",
       "2                 0.514286                       0.383838   \n",
       "3                 0.514286                       0.656566   \n",
       "4                 0.342857                       0.444444   \n",
       "\n",
       "   number of open complaints  number of policies  gender  (suburban,)  \\\n",
       "0                        0.0               0.000       0          1.0   \n",
       "1                        0.0               0.875       0          1.0   \n",
       "2                        0.0               0.125       0          1.0   \n",
       "3                        0.0               0.750       1          1.0   \n",
       "4                        0.0               0.000       1          0.0   \n",
       "\n",
       "   (urban,)  \n",
       "0       0.0  \n",
       "1       0.0  \n",
       "2       0.0  \n",
       "3       0.0  \n",
       "4       0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_cleaning(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a7f30c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
